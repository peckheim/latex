\section{Introduction}
High-performance computing (HPC) systems are relentlessly growing in size to cope with the dramatic increase of working sets in modern workloads with higher performance and energy efficiency. In particular, exploiting parallelism has become key to attain these goals due to the end of Moore's law and Dennard scaling, causing larger numbers of processors within computing nodes (`scale-in') as well as larger numbers of total computing nodes (`scale-out') in HPC systems. These phenomenons put increasing strain on the interconnection networks in all levels of an HPC system's hierarchy (i.e., on-board networks between processors within a node, intra-rack networks between nodes inside a rack, and inter-rack networks between racks) and already have a significant impact on performance, power consumption, and system cost. In fact, it is questionable whether the performance gains of parallel systems can further be exploited without significant networking innovations in place as power consumption and cost of network resources may become prohibitively high. \\
Although the attributes of networks vary based on the hierarchy level, many of their challenges are similar. Making efficient use of network bandwidth is one of the most critical ones as the available network bandwidth directly impacts system cost and power consumption (either through higher number of interconnects in a network topology or higher data rate transceivers). In addition, communication patterns between compute nodes in HPC workloads are typically not evenly distributed, and traffic between certain network nodes (or even the entire network) varies significantly between low-utilization computing-intense and high-utilization communication-intense phases~\cite{gratz2010realistic}. Simultaneously providing enough network bandwidth for high-utilization phases between certain node pairs without wasting energy in low-utilization phases is a considerable challenge. Dynamic voltage and frequency scaling of transceivers partially solves this problem from a power perspective as it allows to adapt a link's data rate to the current load; however, power penalties limit the maximum data rate of transceivers and the resources to have these links in the network topology in the first place still lead to cost overheads for higher-port routers, transceivers, and fibers.  \\
Another challenge is to provide low (zero load) network latency, particularly as network size increases. For instance, in inter-rack communication, latency can become significant to system performance as networks are typically multi-hop fat trees (for load balancing purposes) with switch traversal latencies of 100s of nanoseconds. Clearly, system performance could be significantly improved if fewer hops or even single-hop all-to-all connectivity could be possible. In addition, all-to-all networks can reduce routing overheads (e.g.\ packet sizes and buffers) and the number of switches, enabling lower power and cheaper solutions. \\
In this paper, we address these issues of HPC networks and introduce FlexLION, a bandwidth-reconfigurable all-to-all interconnect fabric that exploits recent advancements in silicon photonic (SiPh) integration and tightly integrates different SiPh components for high energy efficiency. FlexLION solves all the above-mentioned issues by enabling a diameter-1 all-to-all switching fabric for minimal zero-load latency that allows each sender to allocate each available wavelength to any output link (i.e.\ destination) in the network. This allows to adjust the link bandwidth of each outgoing link of a sender to the communication demands it will have with a receiver during a workload. In particular, we make the following novel \textbf{contributions:}
\begin{itemize}
\item Introduce FlexLION, a SiPh all-to-all switching fabric that allows network reconfiguration and bandwidth steering between all connected nodes allowing each sender to flexibly allocate all available wavelengths to its output links based on the traffic demands. 
\item FlexLION tightly integrates an Arrayed Waveguide Grating Router, a color-blind switch, and microring resonators to construct a highly-flexible fabric whose ideal bandwidth utilization allows to reduce network resources (i.e., switches and transceivers) compared to state-of-the Fat Tree networks, lowering energy-per-bit by at least 1.5x (up to 4.1x) while sustaining the same network loads and offering 1.25x lower latency.
\item FlexLION fits on a  $12mm \times 13mm$ interposer and can be reconfigured in less than 10$\mu$s, making it also suitable for other interconnection networks, such as on-board networks between processors and memories. 
\end{itemize}
The remainder of this paper is structured as follows: Section 2 introduces the enabling technologies which form the basis of FlexLION. Section 3 introduces the implementation details of FlexLION and its bandwidth reconfiguration capabilities. In Section 4, we evaluate FlexLION in an inter-rack network scenario and compare it to state-of-the-art networks. Section 5 provide an overview of related work. Section 6 concludes our study. 